{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e0f06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Sun May 22 18:49:50 2022\\n\\n@author: poojap\\n\\nThis is an algorithm detecting eating movements. This is calculated taking many\\ninput actions and descerning eating movements from the rest.\\n\\nIt is an algorithm meant to take movement data from a wearable device in a designated time period of seconds,\\nand determine if the action is eating or non eating.\\n\\nAn accelerometer dataset is provided by watch measurements and gives us x, y, and z coordinates in space.\\n\\nTime datasets are best analyzed by sectioning off a specific portion as a consequence of some number of seconds.\\n\\nWindows are extracted with some overlap to ensure continuity even within consecutive windows. The windows\\nare further analyzed for feature engineering.\\n\\nThe large dataset is minimized via feature engineering. New features are calculated by the extracted windows.\\nThe following features are extracted from each sliding window: \\n    mean, median, mode, standard deviation, absolute average deviation, log average,\\n    square root average, squared average, minimum, maximum, range \\n    \\nThe data is considerably reduced after feature enigneering and the data is more appropriate and adept to \\ntrain a classifier network.\\n\\nA support vector machine (SVM) is used to classify the data as eating and non eating.\\nThe following metrics are used to calculate model performance:\\n        Accuracy, Precision, Recall\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 22 18:49:50 2022\n",
    "\n",
    "@author: poojap\n",
    "\n",
    "This is an algorithm detecting eating movements. This is calculated taking many\n",
    "input actions and descerning eating movements from the rest.\n",
    "\n",
    "It is an algorithm meant to take movement data from a wearable device in a designated time period of seconds,\n",
    "and determine if the action is eating or non eating.\n",
    "\n",
    "An accelerometer dataset is provided by watch measurements and gives us x, y, and z coordinates in space.\n",
    "\n",
    "Time datasets are best analyzed by sectioning off a specific portion as a consequence of some number of seconds.\n",
    "\n",
    "Windows are extracted with some overlap to ensure continuity even within consecutive windows. The windows\n",
    "are further analyzed for feature engineering.\n",
    "\n",
    "The large dataset is minimized via feature engineering. New features are calculated by the extracted windows.\n",
    "The following features are extracted from each sliding window: \n",
    "    mean, median, mode, standard deviation, absolute average deviation, log average,\n",
    "    square root average, squared average, minimum, maximum, range \n",
    "    \n",
    "The data is considerably reduced after feature enigneering and the data is more appropriate and adept to \n",
    "train a classifier network.\n",
    "\n",
    "A support vector machine (SVM) is used to classify the data as eating and non eating.\n",
    "The following metrics are used to calculate model performance:\n",
    "        Accuracy, Precision, Recall\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33680c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.4.3\n",
      "  Downloading matplotlib-3.4.3-cp39-cp39-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.20.3\n",
      "  Using cached numpy-1.20.3-cp39-cp39-macosx_10_9_x86_64.whl (16.1 MB)\n",
      "Collecting pandas==1.3.4\n",
      "  Downloading pandas-1.3.4-cp39-cp39-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit_learn==1.1.1\n",
      "  Using cached scikit_learn-1.1.1-cp39-cp39-macosx_10_13_x86_64.whl (8.6 MB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4->-r ./requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.4.3->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Installing collected packages: numpy, pandas, matplotlib, scikit_learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.4\n",
      "    Uninstalling pandas-1.4.4:\n",
      "      Successfully uninstalled pandas-1.4.4\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "  Attempting uninstall: scikit_learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "deepchem 2.6.1 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.4.3 numpy-1.20.3 pandas-1.3.4 scikit_learn-1.1.1\n"
     ]
    }
   ],
   "source": [
    "# To import all necessary packages, please run: pip install -r ./requirements.txt \n",
    "!pip install -r ./requirements.txt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold \n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import statistics\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13064f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x feature:  8200749 Length of z feature:  8200749\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "def classifyData():\n",
    "    \"\"\"\n",
    "    Takes in numerous .txt files all in one directory, with the filepath: '../DataSetFiles/raw/watch/accel'\n",
    "    Inserts a header in all the .txt files and saves to a new directory: '../DataSetFiles/HeaderFiles'\n",
    "    Concatenates all the data in the new folder 'HeaderFiles' and converts to a pandas dataframe\n",
    "    Returns:\n",
    "        'df': dataframe pulled directly from all concatenated data\n",
    "        'dataframe': df converted to Pandas Dataframe\n",
    "        'dataset': df converted to Numpy Array\n",
    "        'target': Binary class indicating eating (1) or noneating (0)\n",
    "        'x': feature of dataset\n",
    "        'y': feature of dataset\n",
    "        'z':feature of dataset\n",
    "        Binary class indicating eating or noneating: 'target'\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Pathway of current algorithm\n",
    "    dir_path = os.getcwd()\n",
    "    \n",
    "    #Entering one directory above current algorithm to access dataset files\n",
    "    path_parent = os.path.dirname(dir_path)\n",
    "    \n",
    "    #Pathway for dataset all '.txt' files\n",
    "    data ='DataSetFiles/raw/watch/accel'\n",
    "\n",
    "    # Concatening the file paths to access dataset directory\n",
    "    readData = os.path.join(path_parent, data)\n",
    "\n",
    "    #Concatening all the .txt files to read in python\n",
    "    files = os.path.join(readData, \"*.txt\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    joinedfiles = glob.glob(files)\n",
    "\n",
    "    #Header describing each column in .txt file\n",
    "    header = ['SubjectID', 'Class', 'TimeStamp', 'x', 'y', 'z']\n",
    "    \n",
    "    #New directory name to put files with appended header 'HeaderFiles'\n",
    "    newdirname = \"DataSetFiles/HeaderFiles\"\n",
    "    \n",
    "    #Create path filename for new directory\n",
    "    newdir = os.path.join(path_parent, newdirname)\n",
    "    \n",
    "    #Remove header directory if it already exists\n",
    "    if os.path.exists(newdir):\n",
    "        shutil.rmtree(newdir)\n",
    "        \n",
    "    #Make new directory to put files with appended header\n",
    "    os.makedirs(newdir)\n",
    "\n",
    "    #Copy all the files from dataset folder to newly created header folder\n",
    "    for f in joinedfiles:\n",
    "        shutil.copy(f, newdir)\n",
    "    \n",
    "    #Join all the files in new 'HeaderFiles'\n",
    "    joinnew = os.path.join(newdir, \"*.txt\")\n",
    "    \n",
    "    #List of merged files in 'HeaderFiles'\n",
    "    newfiles = glob.glob(joinnew)\n",
    "    \n",
    "    #Loop to read data already existing in file\n",
    "    for filename in joinedfiles:\n",
    "        with open(filename) as infile:\n",
    "            text = infile.read()\n",
    "            reader = csv.reader(infile, delimiter=',' )\n",
    "            \n",
    "    #Loop to input headers followed by initial data\n",
    "    for filename in newfiles:\n",
    "        with open(filename, 'w') as outfile:\n",
    "            # join the headers into a string with commas and add a newline\n",
    "            outfile.write(f\"{','.join(header)}\\n\") \n",
    "            outfile.write(text)\n",
    "    \n",
    "    #Concatenating all the new files with headers from 'HeaderFiles' directory and making a dataframe\n",
    "    df = pd.concat(map(pd.read_csv, newfiles), ignore_index=True)\n",
    "    \n",
    "    #Replace Unknown time values with 0\n",
    "    df['TimeStamp'] = df['TimeStamp'].replace(np.nan, 0)\n",
    "\n",
    "    #Converting df dataframe to a Pandas Dataframe\n",
    "    dataframe = pd.DataFrame(df)\n",
    "    \n",
    "    #Filling any unknown values with 0\n",
    "    dataframe.fillna(0)\n",
    "\n",
    "    #Convert dataframe to a numpy array\n",
    "    dataset = dataframe.to_numpy()\n",
    "    \n",
    "    #Feature x from column 'x'\n",
    "    x = df['x']\n",
    "    \n",
    "    #Feature y from column 'y'\n",
    "    y = df['y']\n",
    "    \n",
    "    #Feature z from column 'z'\n",
    "    z = df['z']\n",
    "    \n",
    "    #Removing all the semicolons from column 'z'\n",
    "    df['z'] = df['z'].str.replace(';','')\n",
    "\n",
    "    #Checking size of each feature to make sure they match\n",
    "    print(\"Length of x feature: \", len(x), \"Length of z feature: \",len(z))\n",
    "    \n",
    "    \n",
    "    #Creating a new column 'binary_eating' to indicate eating or non-eating in a binary fahsion\n",
    "    df.insert(loc=6,\n",
    "          column='binary_eating',\n",
    "          value=0)\n",
    "    \n",
    "    #Creating a list of all the Class values in column 'Class'\n",
    "    classes = df['Class']\n",
    "    \n",
    "    #Copying the data from column 'Class' into column 'binary_eating'    \n",
    "    df['binary_eating'] = df['Class']\n",
    "    \n",
    "    #Classes that indicate eating\n",
    "    eatingClasses = ['H', 'I', 'J', 'K', 'L']\n",
    "    \n",
    "    #Classes that do not indicate eating\n",
    "    noneatingclasses = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T']\n",
    "    \n",
    "    #Replacing all the eating classes with a value of 1\n",
    "    for value in eatingClasses:\n",
    "        df.loc[df['Class'] == value, 'binary_eating'] = 1\n",
    "        \n",
    "    #Replacing all the non-eating classes with a value of 0\n",
    "    for nonval in noneatingclasses:\n",
    "        df.loc[df['Class'] == nonval, 'binary_eating'] = 0\n",
    "\n",
    "    #Creating a list of all the values in the column 'binary_eating'\n",
    "    target = df['binary_eating']\n",
    "    \n",
    "    print(df.head())\n",
    "\n",
    "    return df, dataframe, target, x, y, z \n",
    "\n",
    "\n",
    "#Call the function to print necessary output\n",
    "df, dataframe, target, x, y, z = classifyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56603d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_windows(array, mHZ, seconds, overlap):\n",
    "    \"\"\"\n",
    "    Function to extract windows across the entire dataset.\n",
    "    The window size is described by mHZ multiplied by time second window\n",
    "    The input parameters are the following:\n",
    "        'array': array (dataframe or list) indicating the feature of interest\n",
    "        'mHZ': Signal frequency of how many measurments taken a second\n",
    "        'seconds': How many seconds are taken for the window\n",
    "        'overlap': How much overlap between consecutive windows\n",
    "    The function returns a list of windows sectioning the input feature vector:\n",
    "        'windows': list of arrays of the extracted windows\n",
    "    \"\"\"\n",
    "    #Initializing list holding all windows\n",
    "    windows = []\n",
    "    \n",
    "    #Convert input list/dataframe into numpy array\n",
    "    array = array.to_numpy()\n",
    "    \n",
    "    #Calculate how many datapoints should be in each window\n",
    "    windowSize = mHZ * seconds\n",
    "    \n",
    "    #How many points should overlap between consecutive windows\n",
    "    overlapamt = int(windowSize* overlap)\n",
    "    \n",
    "    #For loop appending each window to the list until no more data points are avaialble\n",
    "    for i in range(len(array)):\n",
    "        window = array[ (overlapamt *i) : (overlapamt*i) +windowSize ]\n",
    "        if len(window)!= 0:\n",
    "            windows.append(window)\n",
    "            \n",
    "    return windows\n",
    "\n",
    "#Call the function to get necessary output\n",
    "\n",
    "# 5 second windows\n",
    "\n",
    "# Cutting the x feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "x_windows = extract_windows(x, 20, 5,  .5)\n",
    "\n",
    "# Cutting the y feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "y_windows = extract_windows(y, 20, 5,  .5)\n",
    "\n",
    "# Cutting the z feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "z_windows = extract_windows(z, 20, 5, .5)\n",
    "\n",
    "# Cutting the target classification list into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "# Done so classes dimensions match that of input features\n",
    "target_window = extract_windows(target, 20, 5, .5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def featureEngineering(x_windows, y_windows, z_windows, target_windows):\n",
    "    \"\"\"\n",
    "    This function engineers features to reduce dataset complexity and \n",
    "    make the data more readable for the model. This reduces the number of data points\n",
    "    needing to be handled, significantly.\n",
    "    The input parameters are the following:\n",
    "        'x_windows': list of windows extracted from x feature\n",
    "        'y_windows': list of windows extracted from y feature\n",
    "        'z_windows': list of windows extracted from z feature\n",
    "        'target_windows': list of windows extracted from target \n",
    "    The following features are engineereed and returned in terms of x, y, and z:\n",
    "        'mean': average of the window\n",
    "        'median': middle value of the entire window\n",
    "        'mode': most common value in the window\n",
    "        'std': standard deviation of the window\n",
    "        'aad': absolute average deviation of the window\n",
    "        'range': difference in values in each window\n",
    "        'minimum': minimum value in the window\n",
    "        'maximum': maximum value in the window\n",
    "        'log': log of every value in the window averaged together\n",
    "        'sqrt': square root of every value in the window averaged together\n",
    "        'square': every value in the window squared and then averaged\n",
    "        'sum': sum of all the values in the window\n",
    "        \n",
    "    \"\"\"\n",
    "    #Convert all string values to float\n",
    "    vector = np.vectorize(float)\n",
    "    for j in range(0, len(x_windows)):\n",
    "        # print(\"array\", j, \": \", x_windows[j])\n",
    "        x_windows[j] = vector(x_windows[j])\n",
    "        y_windows[j] = vector(y_windows[j])\n",
    "        z_windows[j] = vector(z_windows[j])\n",
    "\n",
    "#Engineer the mean\n",
    "    #x feature\n",
    "    x_mean = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xavg = np.mean(x_windows[1])\n",
    "        x_mean.append(xavg)\n",
    "\n",
    "    #y feature\n",
    "    y_mean = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yavg = np.mean(y_windows[i])\n",
    "        y_mean.append(yavg)\n",
    "        \n",
    "    #z feature \n",
    "    z_mean = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zavg = np.mean(z_windows[i])\n",
    "        z_mean.append(zavg)\n",
    "        \n",
    "#Engineer standard deviation\n",
    "    #x std\n",
    "    x_std = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xstd = np.std(x_windows[i])\n",
    "        x_std.append(xstd)\n",
    "    #y std\n",
    "    y_std = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ystd = np.std(y_windows[i])\n",
    "        y_std.append(ystd)\n",
    "    \n",
    "    #z std\n",
    "    z_std = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zstd = np.std(z_windows[i])\n",
    "        z_std.append(zstd)\n",
    "        \n",
    "#Engineer absolute average deviation\n",
    "    x_absavgdev = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xaad = np.mean(np.absolute(x_windows[i] - np.mean(x_windows[i])))\n",
    "        x_absavgdev.append(xaad)\n",
    "    # np.mean(np.absolute(data - np.mean(data)))\n",
    "    \n",
    "    y_absavgdev = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yaad = np.mean(np.absolute(y_windows[i] - np.mean(y_windows[i])))\n",
    "        y_absavgdev.append(yaad)\n",
    "    \n",
    "    z_absavgdev = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zaad = np.mean(np.absolute(z_windows[i] - np.mean(z_windows[i])))\n",
    "        z_absavgdev.append(zaad)\n",
    "        \n",
    "        \n",
    "#Engineer minimum value\n",
    "    x_min = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmin = min(x_windows[i])\n",
    "        x_min.append(xmin)\n",
    "        \n",
    "    y_min = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymin = min(y_windows[i])\n",
    "        y_min.append(ymin)\n",
    "        \n",
    "    z_min = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zmin = min(z_windows[i])\n",
    "        z_min.append(zmin)\n",
    "        \n",
    "#Engineer maximum value\n",
    "    x_max = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmax = min(x_windows[i])\n",
    "        x_max.append(xmax)\n",
    "    \n",
    "    y_max = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymax = min(y_windows[i])\n",
    "        y_max.append(ymax)\n",
    "    \n",
    "    z_max = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zmax = min(z_windows[i])\n",
    "        z_max.append(zmax)\n",
    "#Median feature\n",
    "\n",
    "    x_median = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmedian = statistics.median(x_windows[j])\n",
    "        x_median.append(xmedian)\n",
    "        \n",
    "    y_median = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymedian = statistics.median(y_windows[j])\n",
    "        y_median.append(ymedian)\n",
    "        \n",
    "    z_median = []\n",
    "    print(len(z_windows))\n",
    "    for i in range(len(z_windows)):\n",
    "        zmedian = statistics.median(z_windows[i])\n",
    "        z_median.append(zmedian)\n",
    "        \n",
    "\n",
    "#Mode Feature\n",
    "\n",
    "    x_mode = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xstat = statistics.mode(x_windows[i])\n",
    "        x_mode.append(xstat)\n",
    "        \n",
    "    y_mode = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ystat = statistics.mode(y_windows[i])\n",
    "        y_mode.append(ystat)\n",
    "        \n",
    "    z_mode = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zstat = statistics.mode(z_windows[i])\n",
    "        z_mode.append(zstat)\n",
    "        \n",
    "# Range feature \n",
    "\n",
    "    x_range = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xrange = np.ptp(x_windows[i])\n",
    "        x_range.append(xrange)\n",
    "        \n",
    "    y_range = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yrange = np.ptp(y_windows[i])\n",
    "        y_range.append(yrange)\n",
    "        \n",
    "    z_range = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zrange = np.ptp(z_windows[i])\n",
    "        z_range.append(zrange)\n",
    "        \n",
    "        \n",
    "\n",
    "# Log of data point and then all log(data) averaged together \n",
    "\n",
    "    x_log = []\n",
    "    x_logavg =[]\n",
    "    for i in range(len(x_windows)):\n",
    "        xlogarr = x_windows[i]\n",
    "        for j in range(len(xlogarr)):\n",
    "            value = xlogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            xlogarr[j] = valuesqrt   \n",
    "        x_log.append(xlogarr)\n",
    "        x_loga = np.mean(x_log[i])\n",
    "        x_logavg.append(x_loga)\n",
    "        \n",
    "    \n",
    "    y_log = []\n",
    "    y_logavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ylogarr = y_windows[i]\n",
    "        for j in range(len(ylogarr)):\n",
    "            value = ylogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            ylogarr[j] = valuesqrt  \n",
    "        y_log.append(ylogarr)\n",
    "        y_loga = np.mean(y_log[i])\n",
    "        y_logavg.append(y_loga)\n",
    "        \n",
    "        \n",
    "    z_log = [] \n",
    "    z_logavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zlogarr = z_windows[i]\n",
    "        for j in range(len(zlogarr)):\n",
    "            value = zlogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            zlogarr[j] = valuesqrt          \n",
    "        z_log.append(zlogarr)\n",
    "        z_loga = np.mean(z_log[i])\n",
    "        z_logavg.append(z_loga)            \n",
    "\n",
    "\n",
    "# Square root of each datapoint in the window and averaging them together to one value\n",
    "    # feature x \n",
    "    sqrtx = []\n",
    "    sqrtxavg = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsqrtarr = x_windows[i]\n",
    "        for j in range(len(xsqrtarr)):\n",
    "            value = xsqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            xsqrtarr[j] = valuesqrt\n",
    "        sqrtx.append(xsqrtarr)\n",
    "        x_sqrta = np.mean(sqrtx[i])\n",
    "        sqrtxavg.append(x_sqrta)\n",
    "\n",
    "    # feature y \n",
    "    sqrty = []\n",
    "    sqrtyavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysqrtarr = y_windows[i]\n",
    "        for j in range(len(ysqrtarr)):\n",
    "            value = ysqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            ysqrtarr[j] = valuesqrt        \n",
    "        sqrty.append(ysqrtarr)\n",
    "        y_sqrta = np.mean(sqrty[i])\n",
    "        sqrtyavg.append(y_sqrta)\n",
    "        \n",
    "    # feature z\n",
    "    sqrtz = []\n",
    "    sqrtzavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsqrtarr = z_windows[i]\n",
    "        for j in range(len(zsqrtarr)):\n",
    "            value = zsqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            zsqrtarr[j] = valuesqrt                \n",
    "        sqrtz.append(zsqrtarr)\n",
    "        z_sqrta = np.mean(sqrtz[i])\n",
    "        sqrtzavg.append(z_sqrta)\n",
    "        \n",
    "# Square of each datapoint in the window and averaging them to one value\n",
    "    # x feature\n",
    "    x_sqrd = []\n",
    "    x_sqrdavg = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsqrd = x_windows[i]\n",
    "        for j in range(len(xsqrd)):\n",
    "            xsqrd[j] = np.square(xsqrd[j])\n",
    "        x_sqrd.append(xsqrd)\n",
    "        x_sqrda = np.mean(x_sqrd[i])\n",
    "        x_sqrdavg.append(x_sqrda)\n",
    "\n",
    "    # y feature\n",
    "    y_sqrd = []\n",
    "    y_sqrdavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysqrd = y_windows[i]\n",
    "        for j in range(len(ysqrd)):\n",
    "            ysqrd[j] = np.square(ysqrd[j])\n",
    "        y_sqrd.append(ysqrd)\n",
    "        y_sqrda = np.mean(y_sqrd[i])\n",
    "        y_sqrdavg.append(y_sqrda)\n",
    "\n",
    "    # z feature \n",
    "    \n",
    "    z_sqrd = []\n",
    "    z_sqrdavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsqrd = z_windows[i]\n",
    "        for j in range(len(zsqrd)):\n",
    "            zsqrd[j] = np.square(zsqrd[j])\n",
    "        z_sqrd.append(zsqrd)\n",
    "        z_sqrda = np.mean(z_sqrd[i])\n",
    "        z_sqrdavg.append(z_sqrda)\n",
    "\n",
    "#Sum Feature\n",
    "    # x feature\n",
    "    x_sum = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsum = np.sum(x_windows[i])\n",
    "        x_sum.append(xsum)\n",
    "        \n",
    "    # y feature\n",
    "    y_sum = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysum = np.sum(y_windows[i])\n",
    "        y_sum.append(ysum)\n",
    "    \n",
    "    # z feature\n",
    "    z_sum = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsum = np.sum(z_windows[i])\n",
    "        z_sum.append(zsum)\n",
    "        \n",
    "    #Target mode to reduce complexity and match feature sizes \n",
    "    target_mode = []\n",
    "    for i in range(len(target_windows)):\n",
    "        targetv = statistics.mode(target_windows[i])\n",
    "        target_mode.append(targetv)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "            x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "            x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                y_mode, z_mode, target_mode, x_range, y_range, z_range \\\n",
    "                    , x_logavg, y_logavg,\\\n",
    "                z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                    , x_sum, y_sum, z_sum\n",
    "    \n",
    "#Call the function to get necessary output\n",
    "\n",
    "x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "            x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "            x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                    , x_sum, y_sum, z_sum = featureEngineering(x_windows, y_windows, z_windows, target_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78ee90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x_mean    y_mean    z_mean     x_std     y_std     z_std  x_absavgdev  \\\n",
      "0  4.216959 -8.957493  0.423080  3.963555  4.527882  3.174834     2.840014   \n",
      "1  4.216959 -8.609281  0.307425  3.795516  4.507444  2.952651     2.704928   \n",
      "2  4.216959 -8.327435 -0.034680  3.932344  4.269922  3.040833     3.017129   \n",
      "3  4.216959 -8.239730 -0.172034  3.638833  4.023284  3.183217     2.913560   \n",
      "4  4.216959 -8.264662 -0.235885  3.563615  3.994478  3.080063     2.713212   \n",
      "\n",
      "   y_absavgdev  z_absavgdev     x_min  ...  z_logavg  x_sqrtavg  y_sqrtavg  \\\n",
      "0     3.855340     2.516984 -9.101074  ...  1.472255   1.409860   1.683980   \n",
      "1     3.937267     2.325417 -6.821016  ...  1.408520   1.417566   1.661942   \n",
      "2     3.669260     2.255652 -6.267766  ...  1.334817   1.422222   1.648142   \n",
      "3     3.280723     2.425811 -3.949387  ...  1.391960   1.415428   1.653570   \n",
      "4     3.248390     2.413294 -6.217471  ...  1.411450   1.448140   1.661377   \n",
      "\n",
      "   z_sqrtavg  x_sqrdavg  y_sqrdavg  z_sqrdavg       x_sum       y_sum  \\\n",
      "0   1.181225   2.051263   2.890319   1.472255  205.126302  289.031902   \n",
      "1   1.154629   2.080809   2.822773   1.408520  208.080854  282.277292   \n",
      "2   1.110750   2.114236   2.777341   1.334817  211.423559  277.734071   \n",
      "3   1.136238   2.087990   2.781897   1.391960  208.799020  278.189661   \n",
      "4   1.151388   2.158459   2.797872   1.411450  215.845870  279.787161   \n",
      "\n",
      "        z_sum  \n",
      "0  147.225508  \n",
      "1  140.851962  \n",
      "2  133.481697  \n",
      "3  139.195953  \n",
      "4  141.145025  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "Initial dataframe length:  (8200749, 7)\n",
      "Features dataframe length:  (164015, 33)\n"
     ]
    }
   ],
   "source": [
    "# def AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "#             x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "#             x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "#                 y_mode, z_mode, target_mode, x_range, y_range, z_range , df):\n",
    "\n",
    "def AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "        x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "        x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "            y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "            z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                , x_sum, y_sum, z_sum, df):\n",
    "    \"\"\"\n",
    "    Create a new dataframe with newly engineered features.\n",
    "    This dataframe will be used in the classification model, and allows for more robust analysis.\n",
    "    The input parameters are the following:\n",
    "        All input features are presented in terms of x, y, and z\n",
    "        'mean': average of the window\n",
    "        'median': middle value of the entire window\n",
    "        'mode': most common value in the window\n",
    "        'std': standard deviation of the window\n",
    "        'aad': absolute average deviation of the window\n",
    "        'range': difference in values in each window\n",
    "        'minimum': minimum value in the window\n",
    "        'maximum': maximum value in the window\n",
    "        'log': log of every value in the window averaged together\n",
    "        'sqrt': square root of every value in the window averaged together\n",
    "        'square': every value in the window squared and then averaged\n",
    "        'sum': every value in the window summed\n",
    "        'df': Initial dataframe to compare with features dataframe\n",
    "    This function returns:\n",
    "        'df_features': New dataset with all engineered features\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    #Create a new dataframe with newly engineered feature 'x_mean'\n",
    "    df_features = pd.DataFrame(data = x_mean, columns = ['x_mean'])\n",
    "    \n",
    "    #Append all newly engineered features to the dataframe with an appropriate column name\n",
    "    df_features['y_mean'] = y_mean\n",
    "    df_features['z_mean'] = z_mean\n",
    "    \n",
    "    df_features['x_std'] = x_std\n",
    "    df_features['y_std'] = y_std\n",
    "    df_features['z_std'] = z_std\n",
    "    \n",
    "    df_features['x_absavgdev'] = x_absavgdev\n",
    "    df_features['y_absavgdev'] = y_absavgdev\n",
    "    df_features['z_absavgdev'] = z_absavgdev\n",
    "    \n",
    "    df_features['x_min'] = x_min\n",
    "    df_features['y_min'] = y_min\n",
    "    df_features['z_min'] = z_min\n",
    "    \n",
    "    df_features['x_max'] = x_max\n",
    "    df_features['y_max'] = y_max\n",
    "    df_features['z_max'] = z_max\n",
    "    \n",
    "    df_features['x_median'] = x_median\n",
    "    df_features['y_median'] = y_median\n",
    "    df_features['z_median'] = z_median\n",
    "    \n",
    "    df_features['x_mode'] = x_mode\n",
    "    df_features['y_mode'] = y_mode\n",
    "    df_features['z_mode'] = z_mode\n",
    "    \n",
    "    df_features['x_logavg'] = x_logavg\n",
    "    df_features['y_logavg'] = y_logavg\n",
    "    df_features['z_logavg'] = z_logavg\n",
    "\n",
    "\n",
    "    df_features['x_sqrtavg'] = sqrtxavg\n",
    "    df_features['y_sqrtavg'] = sqrtyavg\n",
    "    df_features['z_sqrtavg'] = sqrtzavg\n",
    "\n",
    "    df_features['x_sqrdavg'] = x_sqrdavg\n",
    "    df_features['y_sqrdavg'] = y_sqrdavg\n",
    "    df_features['z_sqrdavg'] = z_sqrdavg\n",
    "\n",
    "\n",
    "    df_features['x_sum'] = x_sum\n",
    "    df_features['y_sum'] = y_sum\n",
    "    df_features['z_sum'] = z_sum\n",
    "    \n",
    "    \n",
    "    # Comparing the initial dataframe with the feature engineering dataframe\n",
    "    print(df_features.head())\n",
    "    print(df.head())\n",
    "    \n",
    "    #Comparing the length of the initial dataframe with the featured dataframe\n",
    "    print(\"Initial dataframe length: \", np.shape(df))\n",
    "    print(\"Features dataframe length: \", np.shape(df_features))\n",
    "\n",
    "    return df_features\n",
    "\n",
    "# #Call function to return output\n",
    "appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "                x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "                x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                    y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                    z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                        , x_sum, y_sum, z_sum, df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1999e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestData(dataframe, Y):\n",
    "    \"\"\"\n",
    "    Split the dataset to a training and testing set in terms of x (features) and y(classes)\n",
    "    The input parameters are the following:\n",
    "        'dataframe': dataframe used as the x variable to form Xtrain set and Xtest set\n",
    "        'Y': list/array presented as the classification variables to for Ytrain set and Ytest set\n",
    "    Returns:\n",
    "        'X_train': X training set used to train the model with features\n",
    "        'X_test': X testing set used to test model performance\n",
    "        'Y_train': Y training set used to train the model in terms of classification\n",
    "        'Y_test': Y testing set used to test the model in terms of classification\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the classification list into a dataframe so the train_test_split function can utilize appropriately\n",
    "    Y = pd.DataFrame(Y)\n",
    "    \n",
    "    # Split the 'dataframe' and 'Y'into a training and testing dataset\n",
    "    # Outputs a list of the indexes in each set. The test size can be specified with the parameter 'test_size'\n",
    "    X_trainindex , X_testindex, y_trainindex , y_testindex = train_test_split(dataframe.index,Y.index, \\\n",
    "                                                                              test_size=0.2, random_state= None)\n",
    "    \n",
    "    # Take the indexing of X_trainindex and create a new dataframe with the appropriate values for X training set\n",
    "    #    from the input dataframe\n",
    "    X_train = dataframe.iloc[X_trainindex] # return dataframe train\n",
    "    \n",
    "    # Take the indexing of X_testindex and create a new dataframe with the appropriate values \n",
    "    #     for X testing set from input dataframe\n",
    "    X_test = dataframe.iloc[X_testindex]\n",
    "    \n",
    "    # Take the indexing of Y_trainindex and create a new dataframe with the appropriate values \n",
    "    #     for Y training set from input Y\n",
    "    Y_train = Y.iloc[y_trainindex]\n",
    "    \n",
    "    # Take the indexing of Y_testindex and create a new dataframe with the appropriate values \n",
    "    #     for Y testing set from input Y\n",
    "    Y_test = Y.iloc[y_testindex]\n",
    "    \n",
    "    return X_train, X_test, Y_train , Y_test\n",
    "\n",
    "# Call function to return output\n",
    "Xtrain , Xtest, Ytrain, Ytest = TrainTestData(appended_df, target_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7915d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model\n",
      "before accuracy\n",
      "Model Accuracy score:  0.8996128402889979\n",
      "\n",
      "Model Precision Score:  0.7503040834057342\n",
      "\n",
      "Model Recall Score:  0.9537272225289896\n",
      "\n",
      "Model F1 Score:  0.8398735716022367\n"
     ]
    }
   ],
   "source": [
    "def SVM(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    \"\"\" \n",
    "    Classification model named Support Vector Machine that is able to distinguish between two-group classification\n",
    "    problems. After training the model with labeled data for each class/group, the model is able to categorize\n",
    "    new data with predictions.\n",
    "    The input parameters are the following:\n",
    "        'Xtrain': Training set used to train the model with features\n",
    "        'Xtest': Testing set used to test how model performance as control group\n",
    "        'Ytrain': Training set used to test the model's features\n",
    "        'Ytest': Testing set used to test model performance as control group\n",
    "    The model is trained on given input data of the features engineered, and model performance is tested with\n",
    "    known data used for testing. Model performance is evaluated using accuracy, precision, F1, and recall.\n",
    "    Returns:\n",
    "        'accuracy': Percentage reflecting how much of model output is equivalent to real output\n",
    "        'precision': Ratio of True positives by All positives. The ability of the classifier\n",
    "                    to not label a sample positive, when it is negative (label non-eating as non-eating)\n",
    "        'recall': Ratio of True positives by True Positives + False Negatives.\n",
    "                    Ratio of ability of the classifier to find all positive samples (label eating as eating)\n",
    "        'F1': Mean of precision and recall. A value of 1 is optimal, 0 is worst. Calculated by\n",
    "                        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Flattening Ytrain to pass through SVM \n",
    "    Ytrain = Ytrain.values.ravel()\n",
    "    \n",
    "    #Flattening Ytest to pass through SVM\n",
    "    Ytest = Ytest.values.ravel()\n",
    "    \n",
    "    print(\"Check model is running before model initialization\")\n",
    "    # Initalizing model as a Support Vector Classification\n",
    "    model = SVC()\n",
    "\n",
    "    # Fit the model to data\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    # Make predictions with Xtest\n",
    "    y_pred = model.predict(Xtest)\n",
    "    print(\"Check model is running before accuracy\")\n",
    "    #Accuracy Score comparing model predictions with model output\n",
    "    accuracy = accuracy_score(Ytest, y_pred)\n",
    "    print('Model Accuracy score: ', accuracy)    \n",
    "    \n",
    "    #Precision score comparing model predictions with model output with binary classification (Eating/Noneating)\n",
    "    precision = precision_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel Precision Score: ', precision)\n",
    "    \n",
    "    #Recall score comparing model predictions with model output with binary classification (Eating/Noneating)\n",
    "    recall = recall_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel Recall Score: ', recall)\n",
    "    \n",
    "    #F1 or F-score of model\n",
    "    F1 = f1_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel F1 Score: ', F1)\n",
    "    \n",
    "    return accuracy, precision, recall, F1\n",
    "    \n",
    "#Call function to return output\n",
    "accuracy, precision, recall, F1 = SVM(Xtrain, Xtest, Ytrain, Ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a90fad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Starting timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calling classifyData() to read data and assign features and classes\n",
    "    df, dataframe, target, x, y, z = classifyData()\n",
    "    # Cutting the x feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    x_windows = extract_windows(x, 20, 10,  .5)\n",
    "    \n",
    "    # Cutting the y feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    y_windows = extract_windows(y, 20, 10,  .5)\n",
    "    \n",
    "    # Cutting the z feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    z_windows = extract_windows(z, 20, 10, .5)\n",
    "    \n",
    "    # Cutting the target classification list into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    # Done so classes dimensions match that of input features\n",
    "    target_window = extract_windows(target, 20, 10, .5)\n",
    "    \n",
    "    \n",
    "    # Engineering features from the extracted windows, and creating an appropriate categorical class array\n",
    "    \n",
    "    x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "        x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "        x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "            y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "            z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                , x_sum, y_sum, z_sum = featureEngineering(x_windows, y_windows, z_windows, target_window)\n",
    "            \n",
    "    # Creating a new dataframe to be input into the classification model with new features\n",
    "    \n",
    "#     appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "#             x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "#             x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "#                 y_mode, z_mode, target_mode, x_range, y_range, z_range , df)\n",
    "    \n",
    "    appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "                x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "                x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                    y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                    z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                        , x_sum, y_sum, z_sum, df)\n",
    "     \n",
    "    # Splitting the features dataframe and targets to training and testing data\n",
    "    Xtrain , Xtest, Ytrain, Ytest = TrainTestData(appended_df, target_mode)\n",
    "\n",
    "    # Calling Support Vector Machine to create a classification model with training and testing data\n",
    "    accuracy, precision, recall, F1= SVM(Xtrain, Xtest, Ytrain, Ytest)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2594e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x feature:  8200749 Length of z feature:  8200749\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "82008\n",
      "     x_mean    y_mean    z_mean     x_std     y_std     z_std  x_absavgdev  \\\n",
      "0  4.651607 -8.642464  0.194200  3.963171  4.412053  3.116970     2.933002   \n",
      "1  4.651607 -8.296049 -0.135283  3.752544  4.134614  3.062164     2.865794   \n",
      "2  4.651607 -8.423248  0.000647  4.016029  4.152091  2.967418     3.008576   \n",
      "3  4.651607 -8.589977  0.227036  4.285565  4.238946  3.257598     3.331848   \n",
      "4  4.651607 -8.668103  0.294959  4.127995  4.160161  3.573849     3.221035   \n",
      "\n",
      "   y_absavgdev  z_absavgdev     x_min  ...  z_logavg  x_sqrtavg  y_sqrtavg  \\\n",
      "0     3.776829     2.394501 -9.101074  ...  1.403536   1.416041   1.666061   \n",
      "1     3.461022     2.338124 -6.267766  ...  1.373134   1.435181   1.654760   \n",
      "2     3.464138     2.368137 -7.601792  ...  1.417979   1.462584   1.663676   \n",
      "3     3.561217     2.598128 -7.601792  ...  1.479054   1.456319   1.671167   \n",
      "4     3.428339     2.803416 -6.128855  ...  1.523323   1.434331   1.676646   \n",
      "\n",
      "   z_sqrtavg  x_sqrdavg  y_sqrdavg  z_sqrdavg       x_sum       y_sum  \\\n",
      "0   1.145987   2.082749   2.833830   1.403536  416.549861  566.765973   \n",
      "1   1.131069   2.136347   2.787606   1.373134  427.269429  557.521233   \n",
      "2   1.160304   2.199071   2.813803   1.417979  439.814163  562.760696   \n",
      "3   1.185606   2.195215   2.840135   1.479054  439.042930  568.027006   \n",
      "4   1.198886   2.141730   2.857133   1.523323  428.345947  571.426675   \n",
      "\n",
      "        z_sum  \n",
      "0  280.707205  \n",
      "1  274.626722  \n",
      "2  283.595844  \n",
      "3  295.810761  \n",
      "4  304.664594  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "Initial dataframe length:  (8200749, 7)\n",
      "Features dataframe length:  (82008, 33)\n",
      "before model\n",
      "before accuracy\n",
      "Model Accuracy score:  0.8967808803804415\n",
      "\n",
      "Model Precision Score:  0.7557440791799223\n",
      "\n",
      "Model Recall Score:  0.9321996947896228\n",
      "\n",
      "Model F1 Score:  0.8347486578818937\n",
      "--- 823.1942269802094 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc792fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8398735716022367"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3eaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b81ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd60b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
