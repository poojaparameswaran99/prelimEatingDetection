{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e0f06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Sun May 22 18:49:50 2022\\n\\n@author: poojap\\n\\nThis is an algorithm detecting eating movements. This is calculated taking many\\ninput actions and descerning eating movements from the rest.\\n\\nIt is an algorithm meant to take movement data from a wearable device in a designated time period of seconds,\\nand determine if the action is eating or non eating.\\n\\nAn accelerometer dataset is provided by watch measurements and gives us x, y, and z coordinates in space.\\n\\nTime datasets are best analyzed by sectioning off a specific portion as a consequence of some number of seconds.\\n\\nWindows are extracted with some overlap to ensure continuity even within consecutive windows. The windows\\nare further analyzed for feature engineering.\\n\\nThe large dataset is minimized via feature engineering. New features are calculated by the extracted windows.\\nThe following features are extracted from each sliding window: \\n    mean, median, mode, standard deviation, absolute average deviation, log average,\\n    square root average, squared average, minimum, maximum, range \\n    \\nThe data is considerably reduced after feature enigneering and the data is more appropriate and adept to \\ntrain a classifier network.\\n\\nA support vector machine (SVM) is used to classify the data as eating and non eating.\\nThe following metrics are used to calculate model performance:\\n        Accuracy, Precision, Recall\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 22 18:49:50 2022\n",
    "\n",
    "@author: poojap\n",
    "\n",
    "This is an algorithm detecting eating movements. This is calculated taking many\n",
    "input actions and descerning eating movements from the rest.\n",
    "\n",
    "It is an algorithm meant to take movement data from a wearable device in a designated time period of seconds,\n",
    "and determine if the action is eating or non eating.\n",
    "\n",
    "An accelerometer dataset is provided by watch measurements and gives us x, y, and z coordinates in space.\n",
    "\n",
    "Time datasets are best analyzed by sectioning off a specific portion as a consequence of some number of seconds.\n",
    "\n",
    "Windows are extracted with some overlap to ensure continuity even within consecutive windows. The windows\n",
    "are further analyzed for feature engineering.\n",
    "\n",
    "The large dataset is minimized via feature engineering. New features are calculated by the extracted windows.\n",
    "The following features are extracted from each sliding window: \n",
    "    mean, median, mode, standard deviation, absolute average deviation, log average,\n",
    "    square root average, squared average, minimum, maximum, range \n",
    "    \n",
    "The data is considerably reduced after feature enigneering and the data is more appropriate and adept to \n",
    "train a classifier network.\n",
    "\n",
    "A support vector machine (SVM) is used to classify the data as eating and non eating.\n",
    "The following metrics are used to calculate model performance:\n",
    "        Accuracy, Precision, Recall\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33680c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.4.3 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from -r ./requirements.txt (line 1)) (3.4.3)\n",
      "Collecting numpy==1.20.3\n",
      "  Using cached numpy-1.20.3-cp39-cp39-macosx_10_9_x86_64.whl (16.1 MB)\n",
      "Requirement already satisfied: pandas==1.3.4 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from -r ./requirements.txt (line 3)) (1.3.4)\n",
      "Requirement already satisfied: scikit_learn==1.1.1 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from -r ./requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r ./requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4->-r ./requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from scikit_learn==1.1.1->-r ./requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/poojap/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.4.3->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "deepchem 2.6.1 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.20.3\n"
     ]
    }
   ],
   "source": [
    "# To import all necessary packages, please run: pip install -r ./requirements.txt \n",
    "!pip install -r ./requirements.txt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold \n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import statistics\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13064f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x feature:  8200749 Length of z feature:  8200749\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>Class</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>binary_eating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1638</td>\n",
       "      <td>A</td>\n",
       "      <td>1138138097322000</td>\n",
       "      <td>7.302415</td>\n",
       "      <td>-5.419930</td>\n",
       "      <td>4.485872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1638</td>\n",
       "      <td>A</td>\n",
       "      <td>1138138117418000</td>\n",
       "      <td>6.540799</td>\n",
       "      <td>-3.321892</td>\n",
       "      <td>0.71371585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1638</td>\n",
       "      <td>A</td>\n",
       "      <td>1138138137546000</td>\n",
       "      <td>3.264412</td>\n",
       "      <td>-2.723137</td>\n",
       "      <td>0.22513184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1638</td>\n",
       "      <td>A</td>\n",
       "      <td>1138138157791000</td>\n",
       "      <td>1.070574</td>\n",
       "      <td>-3.319497</td>\n",
       "      <td>1.3771362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1638</td>\n",
       "      <td>A</td>\n",
       "      <td>1138138177887000</td>\n",
       "      <td>-1.621428</td>\n",
       "      <td>-3.827241</td>\n",
       "      <td>1.0035132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200744</th>\n",
       "      <td>1638</td>\n",
       "      <td>S</td>\n",
       "      <td>1135293554939000</td>\n",
       "      <td>-4.071533</td>\n",
       "      <td>-7.891590</td>\n",
       "      <td>-1.3747413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200745</th>\n",
       "      <td>1638</td>\n",
       "      <td>S</td>\n",
       "      <td>1135293575112000</td>\n",
       "      <td>-3.877537</td>\n",
       "      <td>-7.846084</td>\n",
       "      <td>-1.4370118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200746</th>\n",
       "      <td>1638</td>\n",
       "      <td>S</td>\n",
       "      <td>1135293595208000</td>\n",
       "      <td>-3.702700</td>\n",
       "      <td>-7.915540</td>\n",
       "      <td>-1.5687379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200747</th>\n",
       "      <td>1638</td>\n",
       "      <td>S</td>\n",
       "      <td>1135293615414000</td>\n",
       "      <td>-3.578159</td>\n",
       "      <td>-8.176597</td>\n",
       "      <td>-1.8130299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200748</th>\n",
       "      <td>1638</td>\n",
       "      <td>S</td>\n",
       "      <td>1135293635510000</td>\n",
       "      <td>-3.499124</td>\n",
       "      <td>-8.449629</td>\n",
       "      <td>-1.8968556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8200749 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SubjectID Class         TimeStamp         x         y           z  \\\n",
       "0             1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
       "1             1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
       "2             1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
       "3             1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
       "4             1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
       "...            ...   ...               ...       ...       ...         ...   \n",
       "8200744       1638     S  1135293554939000 -4.071533 -7.891590  -1.3747413   \n",
       "8200745       1638     S  1135293575112000 -3.877537 -7.846084  -1.4370118   \n",
       "8200746       1638     S  1135293595208000 -3.702700 -7.915540  -1.5687379   \n",
       "8200747       1638     S  1135293615414000 -3.578159 -8.176597  -1.8130299   \n",
       "8200748       1638     S  1135293635510000 -3.499124 -8.449629  -1.8968556   \n",
       "\n",
       "        binary_eating  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "8200744             0  \n",
       "8200745             0  \n",
       "8200746             0  \n",
       "8200747             0  \n",
       "8200748             0  \n",
       "\n",
       "[8200749 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classifyData():\n",
    "    \"\"\"\n",
    "    Takes in numerous .txt files all in one directory, with the filepath: '../DataSetFiles/raw/watch/accel'\n",
    "    Inserts a header in all the .txt files and saves to a new directory: '../DataSetFiles/HeaderFiles'\n",
    "    Concatenates all the data in the new folder 'HeaderFiles' and converts to a pandas dataframe\n",
    "    Returns:\n",
    "        'df': dataframe pulled directly from all concatenated data\n",
    "        'dataframe': df converted to Pandas Dataframe\n",
    "        'dataset': df converted to Numpy Array\n",
    "        'target': Binary class indicating eating (1) or noneating (0)\n",
    "        'x': feature of dataset\n",
    "        'y': feature of dataset\n",
    "        'z':feature of dataset\n",
    "        Binary class indicating eating or noneating: 'target'\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Pathway of current algorithm\n",
    "    dir_path = os.getcwd()\n",
    "    \n",
    "    #Entering one directory above current algorithm to access dataset files\n",
    "    path_parent = os.path.dirname(dir_path)\n",
    "    \n",
    "    #Pathway for dataset all '.txt' files\n",
    "    data ='DataSetFiles/raw/watch/accel'\n",
    "\n",
    "    # Concatening the file paths to access dataset directory\n",
    "    readData = os.path.join(path_parent, data)\n",
    "\n",
    "    #Concatening all the .txt files to read in python\n",
    "    files = os.path.join(readData, \"*.txt\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    joinedfiles = glob.glob(files)\n",
    "\n",
    "    #Header describing each column in .txt file\n",
    "    header = ['SubjectID', 'Class', 'TimeStamp', 'x', 'y', 'z']\n",
    "    \n",
    "    #New directory name to put files with appended header 'HeaderFiles'\n",
    "    newdirname = \"DataSetFiles/HeaderFiles\"\n",
    "    \n",
    "    #Create path filename for new directory\n",
    "    newdir = os.path.join(path_parent, newdirname)\n",
    "    \n",
    "    #Remove header directory if it already exists\n",
    "    if os.path.exists(newdir):\n",
    "        shutil.rmtree(newdir)\n",
    "        \n",
    "    #Make new directory to put files with appended header\n",
    "    os.makedirs(newdir)\n",
    "\n",
    "    #Copy all the files from dataset folder to newly created header folder\n",
    "    for f in joinedfiles:\n",
    "        shutil.copy(f, newdir)\n",
    "    \n",
    "    #Join all the files in new 'HeaderFiles'\n",
    "    joinnew = os.path.join(newdir, \"*.txt\")\n",
    "    \n",
    "    #List of merged files in 'HeaderFiles'\n",
    "    newfiles = glob.glob(joinnew)\n",
    "    \n",
    "    #Loop to read data already existing in file\n",
    "    for filename in joinedfiles:\n",
    "        with open(filename) as infile:\n",
    "            text = infile.read()\n",
    "            reader = csv.reader(infile, delimiter=',' )\n",
    "            \n",
    "    #Loop to input headers followed by initial data\n",
    "    for filename in newfiles:\n",
    "        with open(filename, 'w') as outfile:\n",
    "            # join the headers into a string with commas and add a newline\n",
    "            outfile.write(f\"{','.join(header)}\\n\") \n",
    "            outfile.write(text)\n",
    "    \n",
    "    #Concatenating all the new files with headers from 'HeaderFiles' directory and making a dataframe\n",
    "    df = pd.concat(map(pd.read_csv, newfiles), ignore_index=True)\n",
    "    \n",
    "    #Replace Unknown time values with 0\n",
    "    df['TimeStamp'] = df['TimeStamp'].replace(np.nan, 0)\n",
    "\n",
    "    #Converting df dataframe to a Pandas Dataframe\n",
    "    dataframe = pd.DataFrame(df)\n",
    "    \n",
    "    #Filling any unknown values with 0\n",
    "    dataframe.fillna(0)\n",
    "\n",
    "    #Convert dataframe to a numpy array\n",
    "    dataset = dataframe.to_numpy()\n",
    "    \n",
    "    #Feature x from column 'x'\n",
    "    x = df['x']\n",
    "    \n",
    "    #Feature y from column 'y'\n",
    "    y = df['y']\n",
    "    \n",
    "    #Feature z from column 'z'\n",
    "    z = df['z']\n",
    "    \n",
    "    #Removing all the semicolons from column 'z'\n",
    "    df['z'] = df['z'].str.replace(';','')\n",
    "\n",
    "    #Checking size of each feature to make sure they match\n",
    "    print(\"Length of x feature: \", len(x), \"Length of z feature: \",len(z))\n",
    "    \n",
    "    \n",
    "    #Creating a new column 'binary_eating' to indicate eating or non-eating in a binary fahsion\n",
    "    df.insert(loc=6,\n",
    "          column='binary_eating',\n",
    "          value=0)\n",
    "    \n",
    "    #Creating a list of all the Class values in column 'Class'\n",
    "    classes = df['Class']\n",
    "    \n",
    "    #Copying the data from column 'Class' into column 'binary_eating'    \n",
    "    df['binary_eating'] = df['Class']\n",
    "    \n",
    "    #Classes that indicate eating\n",
    "    eatingClasses = ['H', 'I', 'J', 'K', 'L']\n",
    "    \n",
    "    #Classes that do not indicate eating\n",
    "    noneatingclasses = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T']\n",
    "    \n",
    "    #Replacing all the eating classes with a value of 1\n",
    "    for value in eatingClasses:\n",
    "        df.loc[df['Class'] == value, 'binary_eating'] = 1\n",
    "        \n",
    "    #Replacing all the non-eating classes with a value of 0\n",
    "    for nonval in noneatingclasses:\n",
    "        df.loc[df['Class'] == nonval, 'binary_eating'] = 0\n",
    "\n",
    "    #Creating a list of all the values in the column 'binary_eating'\n",
    "    target = df['binary_eating']\n",
    "    \n",
    "    print(df.head())\n",
    "    \n",
    "    descriptors = df.drop(columns = ['SubjectID', 'Class', 'TimeStamp', 'binary_eating'])\n",
    "    classes = df['binary_eating']\n",
    "\n",
    "    return df, descriptors, classes, target, x, y, z \n",
    "\n",
    "\n",
    "#Call the function to print necessary output\n",
    "df, descriptors, classes, target, x, y, z = classifyData()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72496094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold \n",
    "from functools import reduce\n",
    "\n",
    "def crossValidate(dataframe):\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "    #initializing outside for loop- so can train 4 diff times\n",
    "    acc_score = []\n",
    "    predictprob = []\n",
    "    confmatrices = []\n",
    "    explainedvar = []\n",
    "    balancedaccuracy = []\n",
    "    f1 = []\n",
    "    prec = []\n",
    "    rocauc = []\n",
    "    meanabserr = []\n",
    "\n",
    "    #display confusion matrices\n",
    "    disp = []\n",
    "    count = 0\n",
    "    for train_index , test_index in kf.split(dataframe):\n",
    "      #how many iterations\n",
    "      count +=1\n",
    "      #Split train test each iteratoin\n",
    "      X_train , X_test = dataframe.iloc[train_index,:], dataframe.iloc[test_index, :]\n",
    "      y_train , y_test = classes[train_index] , classes[test_index]\n",
    "    traintestSplit = pd.DataFrame(list(zip(X_train, X_test, y_train, y_test)),\n",
    "               columns =['Xtrain', 'Xtest', 'Ytrain', 'Ytest'])\n",
    "    dfs = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "    #merge all DataFrames into one\n",
    "    final_df = reduce(lambda  left,right: pd.merge(left,right,on=['binary_eating'],\n",
    "                                                how='outer'), dfs)\n",
    "    return X_train, X_test, y_train, y_test, final_df\n",
    "\n",
    "xtrain, xtest, ytrain, ytest, final_df = crossValidate(df)\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56603d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_windows(array, mHZ, seconds, overlap):\n",
    "    \"\"\"\n",
    "    Function to extract windows across the entire dataset.\n",
    "    The window size is described by mHZ multiplied by time second window\n",
    "    The input parameters are the following:\n",
    "        'array': array (dataframe or list) indicating the feature of interest\n",
    "        'mHZ': Signal frequency of how many measurments taken a second\n",
    "        'seconds': How many seconds are taken for the window\n",
    "        'overlap': How much overlap between consecutive windows\n",
    "    The function returns a list of windows sectioning the input feature vector:\n",
    "        'windows': list of arrays of the extracted windows\n",
    "    \"\"\"\n",
    "    #Initializing list holding all windows\n",
    "    windows = []\n",
    "    \n",
    "    #Convert input list/dataframe into numpy array\n",
    "    array = array.to_numpy()\n",
    "    \n",
    "    #Calculate how many datapoints should be in each window\n",
    "    windowSize = mHZ * seconds\n",
    "    \n",
    "    #How many points should overlap between consecutive windows\n",
    "    overlapamt = int(windowSize* overlap)\n",
    "    \n",
    "    #For loop appending each window to the list until no more data points are avaialble\n",
    "    for i in range(len(array)):\n",
    "        window = array[ (overlapamt *i) : (overlapamt*i) +windowSize ]\n",
    "        if len(window)!= 0:\n",
    "            windows.append(window)\n",
    "            \n",
    "    return windows\n",
    "\n",
    "#Call the function to get necessary output\n",
    "\n",
    "# 5 second windows\n",
    "\n",
    "# Cutting the x feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "x_windows = extract_windows(x, 20, 5,  .5)\n",
    "\n",
    "# Cutting the y feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "y_windows = extract_windows(y, 20, 5,  .5)\n",
    "\n",
    "# Cutting the z feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "z_windows = extract_windows(z, 20, 5, .5)\n",
    "\n",
    "# Cutting the target classification list into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "# with 50% overlap\n",
    "# Done so classes dimensions match that of input features\n",
    "target_window = extract_windows(target, 20, 5, .5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f9ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def featureEngineering(x_windows, y_windows, z_windows, target_windows):\n",
    "    \"\"\"\n",
    "    This function engineers features to reduce dataset complexity and \n",
    "    make the data more readable for the model. This reduces the number of data points\n",
    "    needing to be handled, significantly.\n",
    "    The input parameters are the following:\n",
    "        'x_windows': list of windows extracted from x feature\n",
    "        'y_windows': list of windows extracted from y feature\n",
    "        'z_windows': list of windows extracted from z feature\n",
    "        'target_windows': list of windows extracted from target \n",
    "    The following features are engineereed and returned in terms of x, y, and z:\n",
    "        'mean': average of the window\n",
    "        'median': middle value of the entire window\n",
    "        'mode': most common value in the window\n",
    "        'std': standard deviation of the window\n",
    "        'aad': absolute average deviation of the window\n",
    "        'range': difference in values in each window\n",
    "        'minimum': minimum value in the window\n",
    "        'maximum': maximum value in the window\n",
    "        'log': log of every value in the window averaged together\n",
    "        'sqrt': square root of every value in the window averaged together\n",
    "        'square': every value in the window squared and then averaged\n",
    "        'sum': sum of all the values in the window\n",
    "        \n",
    "    \"\"\"\n",
    "    #Convert all string values to float\n",
    "    vector = np.vectorize(float)\n",
    "    for j in range(0, len(x_windows)):\n",
    "        # print(\"array\", j, \": \", x_windows[j])\n",
    "        x_windows[j] = vector(x_windows[j])\n",
    "        y_windows[j] = vector(y_windows[j])\n",
    "        z_windows[j] = vector(z_windows[j])\n",
    "\n",
    "#Engineer the mean\n",
    "    #x feature\n",
    "    x_mean = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xavg = np.mean(x_windows[1])\n",
    "        x_mean.append(xavg)\n",
    "\n",
    "    #y feature\n",
    "    y_mean = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yavg = np.mean(y_windows[i])\n",
    "        y_mean.append(yavg)\n",
    "        \n",
    "    #z feature \n",
    "    z_mean = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zavg = np.mean(z_windows[i])\n",
    "        z_mean.append(zavg)\n",
    "        \n",
    "#Engineer standard deviation\n",
    "    #x std\n",
    "    x_std = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xstd = np.std(x_windows[i])\n",
    "        x_std.append(xstd)\n",
    "    #y std\n",
    "    y_std = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ystd = np.std(y_windows[i])\n",
    "        y_std.append(ystd)\n",
    "    \n",
    "    #z std\n",
    "    z_std = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zstd = np.std(z_windows[i])\n",
    "        z_std.append(zstd)\n",
    "        \n",
    "#Engineer absolute average deviation\n",
    "    x_absavgdev = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xaad = np.mean(np.absolute(x_windows[i] - np.mean(x_windows[i])))\n",
    "        x_absavgdev.append(xaad)\n",
    "    # np.mean(np.absolute(data - np.mean(data)))\n",
    "    \n",
    "    y_absavgdev = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yaad = np.mean(np.absolute(y_windows[i] - np.mean(y_windows[i])))\n",
    "        y_absavgdev.append(yaad)\n",
    "    \n",
    "    z_absavgdev = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zaad = np.mean(np.absolute(z_windows[i] - np.mean(z_windows[i])))\n",
    "        z_absavgdev.append(zaad)\n",
    "        \n",
    "        \n",
    "#Engineer minimum value\n",
    "    x_min = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmin = min(x_windows[i])\n",
    "        x_min.append(xmin)\n",
    "        \n",
    "    y_min = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymin = min(y_windows[i])\n",
    "        y_min.append(ymin)\n",
    "        \n",
    "    z_min = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zmin = min(z_windows[i])\n",
    "        z_min.append(zmin)\n",
    "        \n",
    "#Engineer maximum value\n",
    "    x_max = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmax = min(x_windows[i])\n",
    "        x_max.append(xmax)\n",
    "    \n",
    "    y_max = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymax = min(y_windows[i])\n",
    "        y_max.append(ymax)\n",
    "    \n",
    "    z_max = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zmax = min(z_windows[i])\n",
    "        z_max.append(zmax)\n",
    "#Median feature\n",
    "\n",
    "    x_median = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xmedian = statistics.median(x_windows[j])\n",
    "        x_median.append(xmedian)\n",
    "        \n",
    "    y_median = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ymedian = statistics.median(y_windows[j])\n",
    "        y_median.append(ymedian)\n",
    "        \n",
    "    z_median = []\n",
    "    print(len(z_windows))\n",
    "    for i in range(len(z_windows)):\n",
    "        zmedian = statistics.median(z_windows[i])\n",
    "        z_median.append(zmedian)\n",
    "        \n",
    "\n",
    "#Mode Feature\n",
    "\n",
    "    x_mode = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xstat = statistics.mode(x_windows[i])\n",
    "        x_mode.append(xstat)\n",
    "        \n",
    "    y_mode = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ystat = statistics.mode(y_windows[i])\n",
    "        y_mode.append(ystat)\n",
    "        \n",
    "    z_mode = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zstat = statistics.mode(z_windows[i])\n",
    "        z_mode.append(zstat)\n",
    "        \n",
    "# Range feature \n",
    "\n",
    "    x_range = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xrange = np.ptp(x_windows[i])\n",
    "        x_range.append(xrange)\n",
    "        \n",
    "    y_range = []\n",
    "    for i in range(len(y_windows)):\n",
    "        yrange = np.ptp(y_windows[i])\n",
    "        y_range.append(yrange)\n",
    "        \n",
    "    z_range = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zrange = np.ptp(z_windows[i])\n",
    "        z_range.append(zrange)\n",
    "        \n",
    "        \n",
    "\n",
    "# Log of data point and then all log(data) averaged together \n",
    "\n",
    "    x_log = []\n",
    "    x_logavg =[]\n",
    "    for i in range(len(x_windows)):\n",
    "        xlogarr = x_windows[i]\n",
    "        for j in range(len(xlogarr)):\n",
    "            value = xlogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            xlogarr[j] = valuesqrt   \n",
    "        x_log.append(xlogarr)\n",
    "        x_loga = np.mean(x_log[i])\n",
    "        x_logavg.append(x_loga)\n",
    "        \n",
    "    \n",
    "    y_log = []\n",
    "    y_logavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ylogarr = y_windows[i]\n",
    "        for j in range(len(ylogarr)):\n",
    "            value = ylogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            ylogarr[j] = valuesqrt  \n",
    "        y_log.append(ylogarr)\n",
    "        y_loga = np.mean(y_log[i])\n",
    "        y_logavg.append(y_loga)\n",
    "        \n",
    "        \n",
    "    z_log = [] \n",
    "    z_logavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zlogarr = z_windows[i]\n",
    "        for j in range(len(zlogarr)):\n",
    "            value = zlogarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            zlogarr[j] = valuesqrt          \n",
    "        z_log.append(zlogarr)\n",
    "        z_loga = np.mean(z_log[i])\n",
    "        z_logavg.append(z_loga)            \n",
    "\n",
    "\n",
    "# Square root of each datapoint in the window and averaging them together to one value\n",
    "    # feature x \n",
    "    sqrtx = []\n",
    "    sqrtxavg = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsqrtarr = x_windows[i]\n",
    "        for j in range(len(xsqrtarr)):\n",
    "            value = xsqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            xsqrtarr[j] = valuesqrt\n",
    "        sqrtx.append(xsqrtarr)\n",
    "        x_sqrta = np.mean(sqrtx[i])\n",
    "        sqrtxavg.append(x_sqrta)\n",
    "\n",
    "    # feature y \n",
    "    sqrty = []\n",
    "    sqrtyavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysqrtarr = y_windows[i]\n",
    "        for j in range(len(ysqrtarr)):\n",
    "            value = ysqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            ysqrtarr[j] = valuesqrt        \n",
    "        sqrty.append(ysqrtarr)\n",
    "        y_sqrta = np.mean(sqrty[i])\n",
    "        sqrtyavg.append(y_sqrta)\n",
    "        \n",
    "    # feature z\n",
    "    sqrtz = []\n",
    "    sqrtzavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsqrtarr = z_windows[i]\n",
    "        for j in range(len(zsqrtarr)):\n",
    "            value = zsqrtarr[j]\n",
    "            valuesqrt = math.sqrt(abs(value))\n",
    "            zsqrtarr[j] = valuesqrt                \n",
    "        sqrtz.append(zsqrtarr)\n",
    "        z_sqrta = np.mean(sqrtz[i])\n",
    "        sqrtzavg.append(z_sqrta)\n",
    "        \n",
    "# Square of each datapoint in the window and averaging them to one value\n",
    "    # x feature\n",
    "    x_sqrd = []\n",
    "    x_sqrdavg = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsqrd = x_windows[i]\n",
    "        for j in range(len(xsqrd)):\n",
    "            xsqrd[j] = np.square(xsqrd[j])\n",
    "        x_sqrd.append(xsqrd)\n",
    "        x_sqrda = np.mean(x_sqrd[i])\n",
    "        x_sqrdavg.append(x_sqrda)\n",
    "\n",
    "    # y feature\n",
    "    y_sqrd = []\n",
    "    y_sqrdavg = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysqrd = y_windows[i]\n",
    "        for j in range(len(ysqrd)):\n",
    "            ysqrd[j] = np.square(ysqrd[j])\n",
    "        y_sqrd.append(ysqrd)\n",
    "        y_sqrda = np.mean(y_sqrd[i])\n",
    "        y_sqrdavg.append(y_sqrda)\n",
    "\n",
    "    # z feature \n",
    "    \n",
    "    z_sqrd = []\n",
    "    z_sqrdavg = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsqrd = z_windows[i]\n",
    "        for j in range(len(zsqrd)):\n",
    "            zsqrd[j] = np.square(zsqrd[j])\n",
    "        z_sqrd.append(zsqrd)\n",
    "        z_sqrda = np.mean(z_sqrd[i])\n",
    "        z_sqrdavg.append(z_sqrda)\n",
    "\n",
    "#Sum Feature\n",
    "    # x feature\n",
    "    x_sum = []\n",
    "    for i in range(len(x_windows)):\n",
    "        xsum = np.sum(x_windows[i])\n",
    "        x_sum.append(xsum)\n",
    "        \n",
    "    # y feature\n",
    "    y_sum = []\n",
    "    for i in range(len(y_windows)):\n",
    "        ysum = np.sum(y_windows[i])\n",
    "        y_sum.append(ysum)\n",
    "    \n",
    "    # z feature\n",
    "    z_sum = []\n",
    "    for i in range(len(z_windows)):\n",
    "        zsum = np.sum(z_windows[i])\n",
    "        z_sum.append(zsum)\n",
    "        \n",
    "    #Target mode to reduce complexity and match feature sizes \n",
    "    target_mode = []\n",
    "    for i in range(len(target_windows)):\n",
    "        targetv = statistics.mode(target_windows[i])\n",
    "        target_mode.append(targetv)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "            x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "            x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                y_mode, z_mode, target_mode, x_range, y_range, z_range \\\n",
    "                    , x_logavg, y_logavg,\\\n",
    "                z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                    , x_sum, y_sum, z_sum\n",
    "    \n",
    "#Call the function to get necessary output\n",
    "\n",
    "x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "            x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "            x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                    , x_sum, y_sum, z_sum = featureEngineering(x_windows, y_windows, z_windows, target_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ee90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x_mean    y_mean    z_mean     x_std     y_std     z_std  x_absavgdev  \\\n",
      "0  4.216959 -8.957493  0.423080  3.963555  4.527882  3.174834     2.840014   \n",
      "1  4.216959 -8.609281  0.307425  3.795516  4.507444  2.952651     2.704928   \n",
      "2  4.216959 -8.327435 -0.034680  3.932344  4.269922  3.040833     3.017129   \n",
      "3  4.216959 -8.239730 -0.172034  3.638833  4.023284  3.183217     2.913560   \n",
      "4  4.216959 -8.264662 -0.235885  3.563615  3.994478  3.080063     2.713212   \n",
      "\n",
      "   y_absavgdev  z_absavgdev     x_min  ...  z_logavg  x_sqrtavg  y_sqrtavg  \\\n",
      "0     3.855340     2.516984 -9.101074  ...  1.472255   1.409860   1.683980   \n",
      "1     3.937267     2.325417 -6.821016  ...  1.408520   1.417566   1.661942   \n",
      "2     3.669260     2.255652 -6.267766  ...  1.334817   1.422222   1.648142   \n",
      "3     3.280723     2.425811 -3.949387  ...  1.391960   1.415428   1.653570   \n",
      "4     3.248390     2.413294 -6.217471  ...  1.411450   1.448140   1.661377   \n",
      "\n",
      "   z_sqrtavg  x_sqrdavg  y_sqrdavg  z_sqrdavg       x_sum       y_sum  \\\n",
      "0   1.181225   2.051263   2.890319   1.472255  205.126302  289.031902   \n",
      "1   1.154629   2.080809   2.822773   1.408520  208.080854  282.277292   \n",
      "2   1.110750   2.114236   2.777341   1.334817  211.423559  277.734071   \n",
      "3   1.136238   2.087990   2.781897   1.391960  208.799020  278.189661   \n",
      "4   1.151388   2.158459   2.797872   1.411450  215.845870  279.787161   \n",
      "\n",
      "        z_sum  \n",
      "0  147.225508  \n",
      "1  140.851962  \n",
      "2  133.481697  \n",
      "3  139.195953  \n",
      "4  141.145025  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "   SubjectID Class         TimeStamp         x         y           z  \\\n",
      "0       1638     A  1138138097322000  7.302415 -5.419930    4.485872   \n",
      "1       1638     A  1138138117418000  6.540799 -3.321892  0.71371585   \n",
      "2       1638     A  1138138137546000  3.264412 -2.723137  0.22513184   \n",
      "3       1638     A  1138138157791000  1.070574 -3.319497   1.3771362   \n",
      "4       1638     A  1138138177887000 -1.621428 -3.827241   1.0035132   \n",
      "\n",
      "  binary_eating  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "Initial dataframe length:  (8200749, 7)\n",
      "Features dataframe length:  (164015, 33)\n"
     ]
    }
   ],
   "source": [
    "# def AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "#             x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "#             x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "#                 y_mode, z_mode, target_mode, x_range, y_range, z_range , df):\n",
    "\n",
    "def AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "        x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "        x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "            y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "            z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                , x_sum, y_sum, z_sum, df):\n",
    "    \"\"\"\n",
    "    Create a new dataframe with newly engineered features.\n",
    "    This dataframe will be used in the classification model, and allows for more robust analysis.\n",
    "    The input parameters are the following:\n",
    "        All input features are presented in terms of x, y, and z\n",
    "        'mean': average of the window\n",
    "        'median': middle value of the entire window\n",
    "        'mode': most common value in the window\n",
    "        'std': standard deviation of the window\n",
    "        'aad': absolute average deviation of the window\n",
    "        'range': difference in values in each window\n",
    "        'minimum': minimum value in the window\n",
    "        'maximum': maximum value in the window\n",
    "        'log': log of every value in the window averaged together\n",
    "        'sqrt': square root of every value in the window averaged together\n",
    "        'square': every value in the window squared and then averaged\n",
    "        'sum': every value in the window summed\n",
    "        'df': Initial dataframe to compare with features dataframe\n",
    "    This function returns:\n",
    "        'df_features': New dataset with all engineered features\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    #Create a new dataframe with newly engineered feature 'x_mean'\n",
    "    df_features = pd.DataFrame(data = x_mean, columns = ['x_mean'])\n",
    "    \n",
    "    #Append all newly engineered features to the dataframe with an appropriate column name\n",
    "    df_features['y_mean'] = y_mean\n",
    "    df_features['z_mean'] = z_mean\n",
    "    \n",
    "    df_features['x_std'] = x_std\n",
    "    df_features['y_std'] = y_std\n",
    "    df_features['z_std'] = z_std\n",
    "    \n",
    "    df_features['x_absavgdev'] = x_absavgdev\n",
    "    df_features['y_absavgdev'] = y_absavgdev\n",
    "    df_features['z_absavgdev'] = z_absavgdev\n",
    "    \n",
    "    df_features['x_min'] = x_min\n",
    "    df_features['y_min'] = y_min\n",
    "    df_features['z_min'] = z_min\n",
    "    \n",
    "    df_features['x_max'] = x_max\n",
    "    df_features['y_max'] = y_max\n",
    "    df_features['z_max'] = z_max\n",
    "    \n",
    "    df_features['x_median'] = x_median\n",
    "    df_features['y_median'] = y_median\n",
    "    df_features['z_median'] = z_median\n",
    "    \n",
    "    df_features['x_mode'] = x_mode\n",
    "    df_features['y_mode'] = y_mode\n",
    "    df_features['z_mode'] = z_mode\n",
    "    \n",
    "    df_features['x_logavg'] = x_logavg\n",
    "    df_features['y_logavg'] = y_logavg\n",
    "    df_features['z_logavg'] = z_logavg\n",
    "\n",
    "\n",
    "    df_features['x_sqrtavg'] = sqrtxavg\n",
    "    df_features['y_sqrtavg'] = sqrtyavg\n",
    "    df_features['z_sqrtavg'] = sqrtzavg\n",
    "\n",
    "    df_features['x_sqrdavg'] = x_sqrdavg\n",
    "    df_features['y_sqrdavg'] = y_sqrdavg\n",
    "    df_features['z_sqrdavg'] = z_sqrdavg\n",
    "\n",
    "\n",
    "    df_features['x_sum'] = x_sum\n",
    "    df_features['y_sum'] = y_sum\n",
    "    df_features['z_sum'] = z_sum\n",
    "    \n",
    "    \n",
    "    # Comparing the initial dataframe with the feature engineering dataframe\n",
    "    print(df_features.head())\n",
    "    print(df.head())\n",
    "    \n",
    "    #Comparing the length of the initial dataframe with the featured dataframe\n",
    "    print(\"Initial dataframe length: \", np.shape(df))\n",
    "    print(\"Features dataframe length: \", np.shape(df_features))\n",
    "\n",
    "    return df_features\n",
    "\n",
    "# #Call function to return output\n",
    "appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "                x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "                x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                    y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                    z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                        , x_sum, y_sum, z_sum, df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1999e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestData(dataframe, Y):\n",
    "    \"\"\"\n",
    "    Split the dataset to a training and testing set in terms of x (features) and y(classes)\n",
    "    The input parameters are the following:\n",
    "        'dataframe': dataframe used as the x variable to form Xtrain set and Xtest set\n",
    "        'Y': list/array presented as the classification variables to for Ytrain set and Ytest set\n",
    "    Returns:\n",
    "        'X_train': X training set used to train the model with features\n",
    "        'X_test': X testing set used to test model performance\n",
    "        'Y_train': Y training set used to train the model in terms of classification\n",
    "        'Y_test': Y testing set used to test the model in terms of classification\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the classification list into a dataframe so the train_test_split function can utilize appropriately\n",
    "    Y = pd.DataFrame(Y)\n",
    "    \n",
    "    # Split the 'dataframe' and 'Y'into a training and testing dataset\n",
    "    # Outputs a list of the indexes in each set. The test size can be specified with the parameter 'test_size'\n",
    "    X_trainindex , X_testindex, y_trainindex , y_testindex = train_test_split(dataframe.index,Y.index, \\\n",
    "                                                                              test_size=0.2, random_state= None)\n",
    "    \n",
    "    # Take the indexing of X_trainindex and create a new dataframe with the appropriate values for X training set\n",
    "    #    from the input dataframe\n",
    "    X_train = dataframe.iloc[X_trainindex] # return dataframe train\n",
    "    \n",
    "    # Take the indexing of X_testindex and create a new dataframe with the appropriate values \n",
    "    #     for X testing set from input dataframe\n",
    "    X_test = dataframe.iloc[X_testindex]\n",
    "    \n",
    "    # Take the indexing of Y_trainindex and create a new dataframe with the appropriate values \n",
    "    #     for Y training set from input Y\n",
    "    Y_train = Y.iloc[y_trainindex]\n",
    "    \n",
    "    # Take the indexing of Y_testindex and create a new dataframe with the appropriate values \n",
    "    #     for Y testing set from input Y\n",
    "    Y_test = Y.iloc[y_testindex]\n",
    "    \n",
    "    return X_train, X_test, Y_train , Y_test\n",
    "\n",
    "# Call function to return output\n",
    "Xtrain , Xtest, Ytrain, Ytest = TrainTestData(appended_df, target_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7915d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check model is running before model initialization\n",
      "Check model is running before accuracy\n",
      "Model Accuracy score:  0.9012285461695576\n",
      "\n",
      "Model Precision Score:  0.7575105448911078\n",
      "\n",
      "Model Recall Score:  0.954136398135097\n",
      "\n",
      "Model F1 Score:  0.8445297504798465\n"
     ]
    }
   ],
   "source": [
    "def SVM(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    \"\"\" \n",
    "    Classification model named Support Vector Machine that is able to distinguish between two-group classification\n",
    "    problems. After training the model with labeled data for each class/group, the model is able to categorize\n",
    "    new data with predictions.\n",
    "    The input parameters are the following:\n",
    "        'Xtrain': Training set used to train the model with features\n",
    "        'Xtest': Testing set used to test how model performance as control group\n",
    "        'Ytrain': Training set used to test the model's features\n",
    "        'Ytest': Testing set used to test model performance as control group\n",
    "    The model is trained on given input data of the features engineered, and model performance is tested with\n",
    "    known data used for testing. Model performance is evaluated using accuracy, precision, F1, and recall.\n",
    "    Returns:\n",
    "        'accuracy': Percentage reflecting how much of model output is equivalent to real output\n",
    "        'precision': Ratio of True positives by All positives. The ability of the classifier\n",
    "                    to not label a sample positive, when it is negative (label non-eating as non-eating)\n",
    "        'recall': Ratio of True positives by True Positives + False Negatives.\n",
    "                    Ratio of ability of the classifier to find all positive samples (label eating as eating)\n",
    "        'F1': Mean of precision and recall. A value of 1 is optimal, 0 is worst. Calculated by\n",
    "                        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Flattening Ytrain to pass through SVM \n",
    "    Ytrain = Ytrain.values.ravel()\n",
    "    \n",
    "    #Flattening Ytest to pass through SVM\n",
    "    Ytest = Ytest.values.ravel()\n",
    "    \n",
    "    print(\"Check model is running before model initialization\")\n",
    "    # Initalizing model as a Support Vector Classification\n",
    "    model = SVC()\n",
    "\n",
    "    # Fit the model to data\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    #accuracy\n",
    "    acc = metrics.accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "    #predict probability\n",
    "    predprobs = clf.predict_proba(X_test)[:,1]\n",
    "    predictprob.append(predprobs)\n",
    "\n",
    "      # balanced accuracy \n",
    "\n",
    "    balanceacc = metrics.balanced_accuracy_score(y_test, pred_values)\n",
    "    balancedaccuracy.append(balanceacc)\n",
    "\n",
    "    # graphing predict probability against tanimoto similarity\n",
    "    # fig, pl = plt.subplots(2,2)\n",
    "\n",
    "    # predprobs.scatter(np.min(predprobs,axis=1),\n",
    "    #           np.min(scipy.spatial.distance.cdist(X_train,X_test,metric=\"jaccard\"),axis=0)\n",
    "    # )\n",
    "    # pl.xlabel(\"Predictive uncertainty\")\n",
    "    # pl.ylabel(\"Tanimoto distance\")\n",
    "\n",
    "    #Confusion Matrix\n",
    "    confusionmatrix = metrics.confusion_matrix(y_test, pred_values)\n",
    "    confmatrices.append(confusionmatrix)\n",
    "    dispcm = ConfusionMatrixDisplay(confusion_matrix = confusionmatrix, display_labels = clf.classes_)\n",
    "    disp.append(dispcm)\n",
    "    dispcm.plot() \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # explained variance score\n",
    "\n",
    "    expvar = metrics.explained_variance_score(y_test, pred_values)\n",
    "    explainedvar.append(expvar)\n",
    "\n",
    "    #f1 values\n",
    "    f1val = metrics.f1_score(y_test, pred_values)\n",
    "    f1.append(f1val)\n",
    "\n",
    "    #precision score\n",
    "    precision = metrics.precision_score(y_test, pred_values)\n",
    "    prec.append(precision)\n",
    "\n",
    "    #ROC AUC\n",
    "    rocaucval = metrics.roc_auc_score(y_test, pred_values)\n",
    "    rocauc.append(rocaucval)\n",
    "\n",
    "    #mean absolute value\n",
    "    mae = metrics.mean_absolute_error(y_test, pred_values)\n",
    "    meanabserr.append(mae)\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "    #calibration plot?\n",
    "\n",
    "\n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "    avg_predprob = sum(predictprob)/k\n",
    "    avg_expvar = sum(explainedvar)/k\n",
    "    avg_balancedacc = sum(balancedaccuracy)/k\n",
    "    avg_f1 = sum(f1)/k\n",
    "    avg_prec = sum(prec)/k\n",
    "    avg_rocauc = sum(rocauc)/k\n",
    "    avg_mae = sum(meanabserr)/k\n",
    "\n",
    "    print('accuracy of each fold - {}'.format(acc_score))\n",
    "    print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "\n",
    "    print('prediction probability of each fold - {}'.format(predictprob))\n",
    "    print('Avg predictive probability : {}'.format(avg_predprob))\n",
    "\n",
    "    print('explained variance of each fold - {}'.format(explainedvar))\n",
    "    print('Avg explained variance : {}'.format(avg_expvar))\n",
    "\n",
    "    print('balanced accuracy of each fold - {}'.format(explainedvar))\n",
    "    print('Avg balanced accuracy : {}'.format(avg_balancedacc))\n",
    "\n",
    "    print('f1 value of each fold - {}'.format(f1))\n",
    "    print('Avg f1 : {}'.format(avg_f1))\n",
    "\n",
    "    print('Precision value of each fold - {}'.format(prec))\n",
    "    print('Avg precision : {}'.format(avg_prec))\n",
    "\n",
    "    print('ROC AUC value of each fold - {}'.format(rocauc))\n",
    "    print('Avg ROC AUC : {}'.format(avg_rocauc))\n",
    "\n",
    "    print('Mean absolute error value of each fold - {}'.format(meanabserr))\n",
    "    print('Avg MAE : {}'.format(avg_mae))\n",
    "\n",
    "\n",
    "    #Train the model using the training sets\n",
    "\n",
    "    # Make predictions with Xtest\n",
    "    y_pred = model.predict(Xtest)\n",
    "    print(\"Check model is running before accuracy\")\n",
    "    #Accuracy Score comparing model predictions with model output\n",
    "    accuracy = accuracy_score(Ytest, y_pred)\n",
    "    print('Model Accuracy score: ', accuracy)    \n",
    "\n",
    "    #Precision score comparing model predictions with model output with binary classification (Eating/Noneating)\n",
    "    precision = precision_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel Precision Score: ', precision)\n",
    "\n",
    "    #Recall score comparing model predictions with model output with binary classification (Eating/Noneating)\n",
    "    recall = recall_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel Recall Score: ', recall)\n",
    "\n",
    "    #F1 or F-score of model\n",
    "    F1 = f1_score(Ytest, y_pred, average = 'binary')\n",
    "    print('\\nModel F1 Score: ', F1)\n",
    "\n",
    "    return accuracy, precision, recall, F1\n",
    "\n",
    "#Call function to return output\n",
    "accuracy, precision, recall, F1 = SVM(Xtrain, Xtest, Ytrain, Ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Starting timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calling classifyData() to read data and assign features and classes\n",
    "    df, dataframe, target, x, y, z = classifyData()\n",
    "    # Cutting the x feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    x_windows = extract_windows(x, 20, 10,  .5)\n",
    "    \n",
    "    # Cutting the y feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    y_windows = extract_windows(y, 20, 10,  .5)\n",
    "    \n",
    "    # Cutting the z feature into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    z_windows = extract_windows(z, 20, 10, .5)\n",
    "    \n",
    "    # Cutting the target classification list into windows forming a list of arrays. 20 mHz taken per second for 10 second windows\n",
    "    # with 50% overlap\n",
    "    # Done so classes dimensions match that of input features\n",
    "    target_window = extract_windows(target, 20, 10, .5)\n",
    "    \n",
    "    \n",
    "    # Engineering features from the extracted windows, and creating an appropriate categorical class array\n",
    "    \n",
    "    x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "        x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "        x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "            y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "            z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                , x_sum, y_sum, z_sum = featureEngineering(x_windows, y_windows, z_windows, target_window)\n",
    "            \n",
    "    # Creating a new dataframe to be input into the classification model with new features\n",
    "    \n",
    "#     appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "#             x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "#             x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "#                 y_mode, z_mode, target_mode, x_range, y_range, z_range , df)\n",
    "    \n",
    "    appended_df = AppendNewFeatures(x_mean, y_mean, z_mean, x_std, y_std, z_std, \\\n",
    "                x_absavgdev, y_absavgdev, z_absavgdev, x_min, y_min, z_min, \\\n",
    "                x_max, y_max, z_max, x_median, y_median, z_median, x_mode, \\\n",
    "                    y_mode, z_mode, target_mode, x_range, y_range, z_range, x_logavg, y_logavg,\\\n",
    "                    z_logavg, sqrtxavg, sqrtyavg, sqrtzavg, x_sqrdavg, y_sqrdavg, z_sqrdavg \\\n",
    "                        , x_sum, y_sum, z_sum, df)\n",
    "     \n",
    "    # Splitting the features dataframe and targets to training and testing data\n",
    "    Xtrain , Xtest, Ytrain, Ytest = TrainTestData(appended_df, target_mode)\n",
    "\n",
    "    # Calling Support Vector Machine to create a classification model with training and testing data\n",
    "    accuracy, precision, recall, F1= SVM(Xtrain, Xtest, Ytrain, Ytest)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2594e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc792fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8398735716022367"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3eaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b81ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd60b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
